<!DOCTYPE html>
<html>
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Spencer Dixon</title>
  <link rel="shortcut icon" href="favicon.ico" type="image/vnd.microsoft.icon">
  <link rel="stylesheet" href="/assets/css/style.css" type="text/css">
  <link rel="stylesheet" href="/assets/css/markdown_styles.css" type="text/css">
  <link rel="stylesheet" href="/assets/css/syntax.css" type="text/css">
</head>

  <body>
    <div class="container is-max-desktop">
            <div class="is-flex is-flex-direction-column mt-6 mb-5 is-align-items-center is-justify-content-center">
        <figure class="image is-128x128 mb-2">
          <img class="is-rounded" src="/assets/images/me2.jpg">
        </figure>

        
        <h1 class="title">Spencer Dixon</h1>
        <p class="subtitle is-4">Contract Ruby Developer</p>
      </div>

      <nav class="is-flex is-align-items-center is-justify-content-center is-flex-wrap-wrap mb-6">
    
    
    <a class="button is-rounded m-1" href="/">Home</a>
    
    
    
    <a class="button is-rounded m-1" href="/archive">Archive</a>
    
    
    
    <a class="button is-rounded m-1" href="https://github.com/spencerldixon">Github</a>
    
    
  </nav>

      <div class="mb-6 has-text-centered">
  <h1 class="title is-size-1">Building a Neural Network with Tensorflow</h1>
  <h2 class="subtitle mt-1">07 Nov 2017</h2>
</div>

<div class="card">
  <div class="card-content">
    <p>In my last post we explored the nuts and bolts of how neural networks work by building a simplified neural net using nothing but numpy and Python.</p>

<p>We’ll build a neural network with Tensorflow and teach it to be able to classify images of hand written numbers from 0-9 using the MNIST dataset.</p>

<p><a href="https://www.youtube.com/watch?v=AJsOA4Zl6Io"><img src="https://img.youtube.com/vi/AJsOA4Zl6Io/0.jpg" alt="&quot;It's technology...&quot;" /></a></p>

<p>We’ll start by importing Tensorflow and downloading our dataset which is included in Tensorflow for us…</p>

<h2 id="our-dataset">Our dataset</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Download the mnist dataset and load it into our mnist variable, we'll use one hot encoding...
</span><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">"MNIST_data/"</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>We’ll use one hot encoding which means we’ll convert classifications to a combination of 0’s and 1’s to represent our classification. For example, we could say that True becomes <code class="language-plaintext highlighter-rouge">[0,1]</code>, and False becomes <code class="language-plaintext highlighter-rouge">[1,0]</code>, or Cat becomes <code class="language-plaintext highlighter-rouge">[1,0,0]</code>, while Dog and Mouse become <code class="language-plaintext highlighter-rouge">[0,1,0]</code> and <code class="language-plaintext highlighter-rouge">[0,0,1]</code> respectively.</p>

<p>In our dataset, the position of the 1 will reflect which number it is from 0-9. For example <code class="language-plaintext highlighter-rouge">[0,0,0,1,0,0,0,0,0,0]</code> would represent 3 as it is in the third position (counting from 0).</p>

<p>Our <code class="language-plaintext highlighter-rouge">mnist</code> variable will hold the MNIST data which is split into three parts for us:</p>

<ul>
  <li>Train (55,000 data points of training data accessible via <code class="language-plaintext highlighter-rouge">mnist.train</code>)</li>
  <li>Test (10,000 points of test data accessible via <code class="language-plaintext highlighter-rouge">mnist.test</code>)</li>
  <li>Validation (5,000 points of validation data accessible via <code class="language-plaintext highlighter-rouge">mnist.validation</code>)</li>
</ul>

<p>Train/Test/Validation splits are very important in machine learning. They allow us to keep back a portion of data to test the performance of our model on data it hasn’t seen before for a more reliable accuracy rating. The validation split we won’t use here, but this is usually reserved as a dataset with which to compare the performance of different models, or the same model with different parameters in order to find the best performing model.</p>

<p>Let’s take a look at our data…</p>

<p><img src="/assets/images/building_a_neural_network_with_tensorflow/mnist.png" alt="A single handwritten digit from MNIST as an array of numbers representing pixel colour" /></p>

<h2 id="forward-propagation">Forward Propagation</h2>

<p>Tensorflow works by having you define a computation graph for your data to flow through. You can think of this as like a flow chart; data comes in at the top, and each step we perform an operation and pass it to the next step. Once we’ve defined this in Tensorflow, we can then run it as a session. Tensorflow is great at being able to spread this out across GPUs and other devices for faster processing too should we need it.</p>

<p>As we need to define the computation graph beforehand, we need to create Placeholders which are special variables in Tensorflow that accept incoming data. They’re the gateways to putting data into our neural network. We’ll need two, one to input our dataset of images, and one to input the correct labels. The placeholder for our dataset of images will become the input neurons at the front of our neural network…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We'll input this when we ask TF to run, that's why it's called a placeholder
# These will be our input into the NN
# None means we can input as many as we want, 748 is the flattened array of our 28x28 image.
</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Our flattened array of a 28x28 image
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span> <span class="c1"># Our label (one hot encoded)
</span></code></pre></div></div>

<p>Next, we’ll define and initialise our weights and biases…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialise our weights and bias for our input layer to our hidden layer...
# Our input layer has 784 neurons! That's one per pixel in our flattened array of our image.
</span><span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.03</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W1'</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">300</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b1'</span><span class="p">)</span>

<span class="c1"># And the weights connecting the hidden layer to the output layer...
# We pass our 784 input neurons to a hidden layer of 300 neurons, and then an output of 10 neurons (for our 0-9 classification)
</span><span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">300</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.03</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W2'</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b2'</span><span class="p">)</span>
</code></pre></div></div>

<p>Biases are just like another set of neurons to give us a little more variance to tune in our network. Just like before, we’ll pass our inputs through the first layer, multiplying our weights and adding a bias. Then we’ll apply an activation function. This time we’ll use a RELU activation function instead of the sigmoid we used previously (RELU’s are the trendy activation function right now). Our final prediction will be activated using a softmax function which will convert our prediction to between 0 - 1 for our output.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hidden_out</span>           <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">W1</span><span class="p">),</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">hidden_out_activated</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden_out</span><span class="p">)</span>

<span class="n">output</span>              <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden_out_activated</span><span class="p">,</span> <span class="n">W2</span><span class="p">),</span> <span class="n">b2</span><span class="p">)</span>
<span class="n">predictions</span>         <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="backpropagation">Backpropagation</h2>

<p>We’ll define our cost function next, this is where things start to get a little easier by using Tensorflow. As Tensorflow has gone through our forward prop, it automatically knows how to do backprop! We just have to define which cost function we’ll be using and how we want to minimise it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>

<p>We’ll need to define our hyperparameters. Hyperparameters are like the tuning knobs of neural network, they’re various parameters that control things like how fast our network will learn and end up affecting the final accuracy of our network. They’re called hyperparameters as they’re the parameters that affect how our network learns its parameters (the optimal weights and biases).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">epochs</span>        <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span>    <span class="o">=</span> <span class="mi">100</span>
</code></pre></div></div>

<p>Instead of calculating the gradients ourselves like last time, Tensorflow let’s us just specify which way we’ll be optimising our algorithm. We’ll use Gradient Descent like last time, although there are other options available to us which do the same minimizing of a cost function in different ways. We’ll specify gradient descent as the way we’re optimising, and then give it the cost function we want to minimise using gradient descent.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimiser</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="training">Training</h2>

<p>We have to initialise the variables we defined in Tensorflow. We’ll also come up with a way to accurately measure how if our prediction was correct…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1"># Define an accuracy assessment operation
</span><span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
</code></pre></div></div>

<p>Finally we can run our Tensorflow session and train our network. After our training loops, we’ll pass in the unseen test dataset to see how well our network did.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">batch_xs</span><span class="p">,</span> <span class="n">batch_ys</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">optimiser</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">inputs</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">batch_ys</span><span class="p">})</span>

    <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">inputs</span><span class="p">:</span> <span class="n">mnist</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">mnist</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">labels</span><span class="p">}))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mf">0.9682</span>
</code></pre></div></div>

<p>A 0.9682% accuracy isn’t awful for our first network! But this can be improved quite easily. Try tuning the network above to see how you can increase performance. You may want to try tweaking the hyperparameters, changing the activation functions or optimiser. The best algorithms can get over 99% accuracy on this task!</p>


  </div>
</div>

      <p class="has-text-grey has-text-centered my-5 is-size-7">Made with Prosecco, a Jekyll theme I made while drinking Prosecco.</p>
    </div>

    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.5.3/modernizr.min.js" type="text/javascript"></script>
  </body>
</html>





