I"r<p>Reinforcement Learning is a field of Machine Learning which aims to develop intelligences that learn through trial and error by exploring and interacting with their environment.</p>

<p>You may have seen exciting demos of an AI learning to play video games or robot arms learning to manipulate objects or mimic tasks.</p>

<p>In this post, we’ll look at a very basic approach to Reinforcement Learning which we can use to learn to play very simple games. It’s a limited approach and we’ll quickly find problems with it, but that will set us up nicely for a follow up post on how we can improve on this technique</p>

<h2 id="state-actions-and-rewards">State, Actions and Rewards</h2>

<p>Reinforcement Learning (RL) consists of two actors: an agent (our model / algorithm), and the environment (the game we’re playing in this case). Our agent seeks to develop an optimal policy for interacting with the environment that maximises the cumulative reward over time.</p>

<p>Our environment is represented by a series of <em>states</em>. If we think of a grid with a single playing piece, we can transition states by moving our piece around the board. Each move takes us to a different possible state that the game can be in. We have several different things we can do to our piece, known as <em>actions</em>. We can move up, down, left, or right. Each of these actions transitions us to a new state and brings us one step closer to, or further away from, our goal state (usually a state that will win the game).</p>

<p>When we take an action in our game, we receive some feedback, usually in the form of a score. It’s this <em>reward</em> that we seek to maximise over our time playing the game.</p>

<p>In short, our agent interacts with our environment by choosing <em>actions</em> (a) to take at a particular <em>state</em> (s) with the intention of maximising future <em>rewards</em> (r) received. It’s this mapping of what action we should take at what particular state that’s the problem that we need to solve and is referred to as the <em>policy</em> (π), with the optimal policy being denoted as <em>π&amp;ast;</em></p>

<p><img src="/assets/images/a_primer_on_reinforcement_learning/cycle.png" alt="The Reinforcement Learning Cycle" /></p>

<h2 id="the-q-table">The Q-Table</h2>

<p>There are a lot of different techniques that we can use to get our model to converge on an optimal policy but in this post, we’re going to go for the one which allows us to get something up and running quickly.</p>

<p>Q-Learning is a technique that seeks to assess the <em>quality</em> of taking a given action at a given state. If we drew up a table, with all our game states as rows, and all our possible actions as columns. We could use it as a cheat sheet to keep track of the rewards we recieve for a state action pair over time. If we move left at state 1, and receive a reward of +1, we’ll write that down in our table. Over time, we’ll begin to build up a picture of the quality of every action we can take at every state, and could easily select the actions with the biggest score, or Q-value, to cheat our way to the end of the game.</p>

<p>But this raises two important questions, how do we update our score for each state action pair? And, how do we move around our game when we have no values in our table yet?</p>

<h2 id="the-bellman-equation">The Bellman Equation</h2>

<p>The Bellman Equation provides the foundation for assessing the quality of taking a given action at a given state. If we know the rewards at each state in the game, for example, landing on a safe tile is +1 point, and losing the game is -10 points, we can use the Bellman equation to calculate the optimal Q-value for each state action pair.</p>

<p>We’ll iteratively update our Q-values in our Q-table until we converge to the optimal policy, seeking to reduce the loss between our Q-value, and our optimal Q-value.</p>

<p>Firstly, we’ll need to take an action from our state. Then we’ll receive our reward, along with a new state. We can use this new information, along with the information of the action we took at the previous state to assess how good or bad our move was.</p>

<p>What if we come across a reward for a state action pair we’ve already seen? Over writing previous Q-values would lose valuable information about previous game plays. Instead, we can use a learning rate to update our Q-value. The higher the learning rate, the more quickly our agent will adopt new values and disregard previous values. With a learning rate of 1.0, our agent would simply rewrite all the old values with new values each time.</p>

<p><img src="/assets/images/a_primer_on_reinforcement_learning/bellman.png" alt="The Bellman Equation" /></p>

<h2 id="exploration-vs-exploitation">Exploration vs Exploitation</h2>

<p>Imagine our agent is playing a game with an empty Q table. Initially, it’ll be fairly useless, so using it as a policy to follow won’t get us anywhere. In fact, when we do build up values in our Q table, we’ll want to avoid using them too early before they’ve converged. If we exploit our table too soon, we’ll get stuck at early rewards and possibly miss out on taking larger rewards that could benefit us long term.</p>

<p>Instead, we’ll want to strike a decaying balance between how much we want to explore the games state spaces to find new rewards, and how much we want to exploit our Q table for the correct answers. This is referred to as exploration vs exploitation.</p>

<p>We’ll do this with a strategy known as <em>epsilon greedy</em>. We’ll set an <code class="language-plaintext highlighter-rouge">epsilon</code> number, which represents our probability of choosing exploration vs exploitation. With epsilon set to 1.0, we have a 100% chance that we’ll explore our environment, and thus take actions entirely at random. With epsilon at 0, we’ll exploit our Q-table and select the action with the highest value. As we iterate through our training, we’ll slowly decay epsilon by <code class="language-plaintext highlighter-rouge">gamma</code>, a small number, that will make it less and less probable over time that we’ll explore our environment by selecting random actions.</p>

<h2 id="frozen-lake">Frozen Lake</h2>

<p>Frozen Lake is a game where we have to navigate a 4x4 grid of tiles, each of a different surface type. S is our starting point, G is our goal point, F are frozen tiles (safe to step on) and H are holes which we can fall into and lose the game. We’ll train an agent to safely navigate from the starting tile to our goal tile using Q-Learning.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SFFF</span>
<span class="n">FHFH</span>
<span class="n">FFFH</span>
<span class="n">HFFG</span>
</code></pre></div></div>

<p>We’ll first import OpenAI’s gym library, which will give us the FrozenLake game, with a nice wrapper to be able to access actions and the state space. We’ll also require numpy and random.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">gym</span>
</code></pre></div></div>

<p>Next we’ll start up our FrozenLake game and assign the environment to a variable we can use later on</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"FrozenLake-v0"</span><span class="p">)</span>
</code></pre></div></div>

<p>In order to form our Q table, we’ll need to know the number of possible actions and states. We’ll then create an empty table, initialised with zeros at the moment, that we can later update throughout our training with our Q values, much like how we update weights in a neural network. Our Q table acts like a cheat sheet, reflecting the quality of taking that particular action, at a particular state.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">action_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">state_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span>

<span class="n">qtable</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">qtable</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]]</span>
</code></pre></div></div>

<p>Let’s set some hyperparameters…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">episodes</span>       <span class="o">=</span> <span class="mi">10000</span>      <span class="c1"># Total episodes
</span><span class="n">max_steps</span>      <span class="o">=</span> <span class="mi">99</span>         <span class="c1"># Max moves per episode - stops us exploring infinitely
</span>
<span class="n">learning_rate</span>  <span class="o">=</span> <span class="mf">0.8</span>        <span class="c1"># Learning rate
</span><span class="n">gamma</span>          <span class="o">=</span> <span class="mf">0.95</span>       <span class="c1"># Discounting rate
</span>
<span class="n">epsilon</span>        <span class="o">=</span> <span class="mf">1.0</span>        <span class="c1"># Exploration vs exploitation rate
</span><span class="n">decay_rate</span>     <span class="o">=</span> <span class="mf">0.001</span>      <span class="c1"># How much we want to decay our exploration vs exploitation rate
</span></code></pre></div></div>

<p>We’ll write some helper functions that will make it easier to understand what our code is doing without getting caught up in the formulas.</p>

<p>Firstly, we’ll need a function that, over time, will make a gradual progression from exploration of our environment, to exploitation by utilising our Q table. We’ll use epsilon to denote our exploration vs exploitation rate, and reduce it by our <code class="language-plaintext highlighter-rouge">decay_rate</code> for every episode.</p>

<p><code class="language-plaintext highlighter-rouge">max_epsilon</code> is the largest our epsilon can be and represents a full 100% chance we’ll explore our environment. Conversely, <code class="language-plaintext highlighter-rouge">min_epsilon</code> represents a 100% chance that we’ll exploit our Q table for the correct answers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reduce_epsilon</span><span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">min_epsilon</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">min_epsilon</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_epsilon</span> <span class="o">-</span> <span class="n">min_epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">decay_rate</span> <span class="o">*</span> <span class="n">episode</span><span class="p">)</span>
</code></pre></div></div>

<p>Epsilon will be used in selecting wether we want to explore our environment, which we’ll do by selecting an action at random, or wether we want to exploit our learned Q-table, which we can do by selecting the action with the highest Q value.</p>

<p>Since this is fairly simply logic, we can code this into another helper function which will help simplify our code…</p>

<p>Here we take in some information like our <code class="language-plaintext highlighter-rouge">epsilon</code>, our <code class="language-plaintext highlighter-rouge">qtable</code>, <code class="language-plaintext highlighter-rouge">state</code> and the <code class="language-plaintext highlighter-rouge">env</code> (environment) and generate a random number. If our number is larger than epsilon, we’ll choose to exploit our Q-table by selecting the action with the highest Q-value. If our number is lower than epsilon, we’ll explore our environment further by selecting a random action from our action space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># Exploitation
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Exploration
</span>        <span class="k">return</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
</code></pre></div></div>

<p>Lastly, we’ll need a function to update the values in our Q-table based upon the Bellman equation given our previous state, action taken, reward, and new state…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_qtable</span><span class="p">(</span><span class="n">qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="c1"># Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
</span>    <span class="c1"># qtable[new_state,:] : all the actions we can take from new state
</span>
    <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">new_state</span><span class="p">,</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">qtable</span>
</code></pre></div></div>

<h2 id="training-our-q-table">Training our Q-Table</h2>

<p>Next up is the bulk of our code. This is where we’ll put the pieces together, train our agent, and populate our Q-table.</p>

<p>For every episode in our total number of <code class="language-plaintext highlighter-rouge">episodes</code>, we’ll firstly reset our environment and a few variables which will keep track of game play for that particular run. For each step, we’ll use our <code class="language-plaintext highlighter-rouge">select_action()</code> function to choose an action, either at random (exploration) or from our Q-table (exploitation). This rate will gradually ramp towards more and more exploitation over time as we build up our Q-table.</p>

<p>We’ll then take our action and observe the reward and new state returned, which we’ll use to update our Q-table. Finally, we’ll we’ll set our <code class="language-plaintext highlighter-rouge">state</code> to be the <code class="language-plaintext highlighter-rouge">new_state</code> that we received by taking an action, reduce <code class="language-plaintext highlighter-rouge">epsilon</code> to lean slightly more towards exploitation, add our reward to a list so that we can keep track of how we’re improving over time, and start the cycle over again until we reach some terminal state in our game (we fall into a hole, or win the game).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="c1"># Reset the environment
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">total_rewards</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
        <span class="c1"># Use epsilon to pick an action, either at random, or from our q-table
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>

        <span class="c1"># Take the action and observe the new state and reward
</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># Update our Q-table to take note of how valuable the action according to the reward we got
</span>        <span class="n">qtable</span> <span class="o">=</span> <span class="n">update_qtable</span><span class="p">(</span><span class="n">qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

        <span class="c1"># Set state to the new state we received (where we moved to)
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

        <span class="n">total_rewards</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="c1"># If the game is over, exit the loop, back to a new training loop
</span>        <span class="k">if</span> <span class="n">done</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">reduce_epsilon</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>

    <span class="n">rewards</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Score over time: "</span> <span class="o">+</span>  <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">/</span><span class="n">total_episodes</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">qtable</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="playing-the-game">Playing the Game</h2>

<p>Once we’ve populated our Q-table, we can exploit it to play the game successfully. As we’re simply following our Q-table, we no longer have to deal with updating our table, or dealing with our exploration vs exploitation trade off. We can simply just follow the policy of selecting the highest value at a given state. With a well trained Q-table, our values should closely reflect the maximum expected reward over time by taking that particular action at that particular state.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Playing Round #"</span><span class="p">,</span> <span class="n">episode</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
        <span class="c1"># Select the action with the highest reward
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span>

        <span class="c1"># Return our new state and reward
</span>        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># If the game is finished, we'll print our environment to see if we fell into a hole, or ended on our goal tile
</span>            <span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>

            <span class="c1"># We print the number of steps it took.
</span>            <span class="k">print</span><span class="p">(</span><span class="s">"Steps taken:"</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

<span class="n">env</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="summary">Summary</h2>

<p>And there we have it! We successfully trained a model to learn play the Frozen Lake game by exploring the environment itself, and learning through trial and error.</p>

<p>But what happens when we want to play a more complex game with millions of possible states? Unfortunately, as your state space grows, you very quickly out grow the feasibility of using a Q-table. It would take millions of iterations to even begin to explore all the possible state spaces and build up an accurate Q-table.</p>

<p>Instead of creating a cheat sheet that we can look up every possible value for every possible action in every possible state, what if we could just simplify by having a function that approximates the Q-value for a given state action pair?</p>

<p>If you have read previous posts, you may be familiar with one tool that we can use for function approximation; the neural network.</p>

<p>In a future post we’ll look at how we can improve on our reinforcement learning agent by using neural networks to apply our techniques to larger state spaces and more complex games with Deep Q-Learning.</p>
:ET